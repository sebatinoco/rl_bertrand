{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 500/500 [00:04<00:00, 104.78it/s]\n"
     ]
    }
   ],
   "source": [
    "#from envs.LinearBertrandInflation_final3 import LinearBertrandEnv\n",
    "from envs.LinearBertrandInflation_profe import LinearBertrandEnv\n",
    "from envs.BertrandInflation_profe import BertrandEnv\n",
    "from agents.sac_moving4 import SACAgent\n",
    "from replay_buffer_final import ReplayBuffer\n",
    "from tqdm import tqdm\n",
    "import numpy as np\n",
    "\n",
    "N = 2\n",
    "k = 10\n",
    "rho = 5e-3\n",
    "#total_timesteps = 30_000\n",
    "total_timesteps = 500\n",
    "episodes = 1\n",
    "inflation_start = 0\n",
    "\n",
    "#env = LinearBertrandEnv(N = N, k = k, rho = rho, v = int(k * 1.5), timesteps = total_timesteps, moving_dim = 200, use_moving_avg=True, inflation_start=inflation_start, max_var = 2.0)\n",
    "env = BertrandEnv(N = N, k = k, rho = rho, v = int(k * 1.5), timesteps = total_timesteps, use_moving_avg=True, inflation_start=inflation_start, max_var = 2.0)\n",
    "dim_states = N * k + k + 1\n",
    "agents = [SACAgent(dim_states, 1, gamma=0.95) for agent in range(env.N)]\n",
    "\n",
    "buffer = ReplayBuffer(dim_states=dim_states, N = env.N, sample_size = 256, buffer_size = 1000000)\n",
    "\n",
    "prices_history = np.zeros((episodes, total_timesteps, N))\n",
    "actions_history = np.zeros((episodes, total_timesteps, N))\n",
    "costs_history = np.zeros((episodes, total_timesteps))\n",
    "monopoly_history = np.zeros((episodes, total_timesteps))\n",
    "nash_history = np.zeros((episodes, total_timesteps))\n",
    "rewards_history = np.zeros((episodes, total_timesteps, N))\n",
    "delta_history = np.zeros((episodes, total_timesteps))\n",
    "quantities_history = np.zeros((episodes, total_timesteps, N))\n",
    "pi_N_history = np.zeros((episodes, total_timesteps))\n",
    "pi_M_history = np.zeros((episodes, total_timesteps))\n",
    "A_history = np.zeros((episodes, total_timesteps))\n",
    "\n",
    "for episode in range(episodes):\n",
    "    ob_t = env.reset()\n",
    "    for timestep in tqdm(range(total_timesteps)):\n",
    "        \n",
    "        actions = [agent.select_action(ob_t) for agent in agents]\n",
    "        \n",
    "        ob_t1, rewards, done, _ = env.step(actions)\n",
    "        \n",
    "        experience = (ob_t, actions, rewards, ob_t1, done)\n",
    "        buffer.store_transition(*experience)\n",
    "        \n",
    "        if timestep > buffer.sample_size:\n",
    "            for agent_idx in range(N):\n",
    "                agent = agents[agent_idx]\n",
    "                sample = buffer.sample(agent_idx)\n",
    "                agent.update(*sample)\n",
    "        \n",
    "        ob_t = ob_t1\n",
    "        \n",
    "    # store metrics\n",
    "    prices_history[episode] = np.array(env.prices_history)[-total_timesteps:]\n",
    "    actions_history[episode] = np.array(env.action_history)[-total_timesteps:]\n",
    "    costs_history[episode] = np.array(env.costs_history)[-total_timesteps:]\n",
    "    monopoly_history[episode] = np.array(env.monopoly_history)[-total_timesteps:]\n",
    "    nash_history[episode] = np.array(env.nash_history)[-total_timesteps:]\n",
    "    rewards_history[episode] = np.array(env.rewards_history)[-total_timesteps:]\n",
    "    delta_history[episode] = np.array(env.metric_history)[-total_timesteps:]\n",
    "    quantities_history[episode] = np.array(env.quantities_history)[-total_timesteps:]\n",
    "    pi_N_history[episode] = np.array(env.pi_N_history)[-total_timesteps:]\n",
    "    pi_M_history[episode] = np.array(env.pi_M_history)[-total_timesteps:]\n",
    "    A_history[episode] = np.array(env.A_history)[-total_timesteps:]\n",
    "    \n",
    "prices_history = np.mean(prices_history, axis = 0)\n",
    "actions_history = np.mean(actions_history, axis = 0)\n",
    "costs_history = np.mean(costs_history, axis = 0)\n",
    "monopoly_history = np.mean(monopoly_history, axis = 0)\n",
    "nash_history = np.mean(nash_history, axis = 0)\n",
    "rewards_history = np.mean(rewards_history, axis = 0)\n",
    "delta_history = np.mean(delta_history, axis = 0)\n",
    "quantities_history = np.mean(quantities_history, axis = 0)\n",
    "pi_N_history = np.mean(pi_N_history, axis = 0)\n",
    "pi_M_history = np.mean(pi_M_history, axis = 0)\n",
    "A_history = np.mean(A_history, axis = 0) # equal disposition to pay"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "results = pd.DataFrame({'costs': costs_history,\n",
    "                        'pi_N': pi_N_history,\n",
    "                        'pi_M': pi_M_history,\n",
    "                        'delta': delta_history,\n",
    "                        'p_nash': nash_history,\n",
    "                        'p_monopoly': monopoly_history,\n",
    "                        'A': A_history,\n",
    "                        })\n",
    "\n",
    "for agent in range(env.N):\n",
    "    results[f'actions_{agent}'] = actions_history[:, agent]\n",
    "    results[f'prices_{agent}'] = prices_history[:, agent]\n",
    "    results[f'quantities_{agent}'] = quantities_history[:, agent]\n",
    "    results[f'rewards_{agent}'] = rewards_history[:, agent]\n",
    "\n",
    "results.to_csv(f'test.csv', index = False, sep = ';', encoding = 'utf-8-sig')\n",
    "results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils.plot_metrics import get_rolling\n",
    "\n",
    "window_size = 1000\n",
    "df_plot = pd.read_csv('test.csv', sep = ';', encoding = 'utf-8-sig')\n",
    "\n",
    "actions_cols = [col for col in df_plot.columns if 'actions' in col]\n",
    "price_cols = [col for col in df_plot.columns if 'prices' in col]\n",
    "rewards_cols = [col for col in df_plot.columns if 'rewards' in col]\n",
    "quantities_cols = [col for col in df_plot.columns if 'quantities' in col]\n",
    "\n",
    "n_agents = len(actions_cols)\n",
    "\n",
    "df_plot['avg_actions'] = df_plot[actions_cols].mean(axis = 1)\n",
    "df_plot['avg_prices'] = df_plot[price_cols].mean(axis = 1)\n",
    "df_plot['avg_rewards'] = df_plot[rewards_cols].mean(axis = 1)\n",
    "df_plot['avg_quantities'] = df_plot[quantities_cols].mean(axis = 1)\n",
    "avg_cols = [col for col in df_plot.columns if 'avg' in col]\n",
    "\n",
    "window_cols = price_cols + rewards_cols + quantities_cols + avg_cols + ['delta']\n",
    "for col in window_cols:\n",
    "    df_plot[col] = get_rolling(df_plot[col], window_size = window_size)\n",
    "\n",
    "df_plot.tail()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.figure(figsize = (12, 4))\n",
    "for agent in range(n_agents):\n",
    "    price_serie = df_plot[f'prices_{agent}']\n",
    "    plt.plot(price_serie, label = f'Agent {agent}')\n",
    "plt.plot(df_plot['p_monopoly'], color = 'red', label = 'Monopoly price')\n",
    "plt.plot(df_plot['p_nash'], color = 'green', label = 'Nash price')\n",
    "plt.xlabel('Timesteps')\n",
    "plt.ylabel('Prices')\n",
    "plt.legend()\n",
    "plt.savefig('plot.png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.figure(figsize = (12, 4))\n",
    "plt.plot(df_plot['avg_prices'], label = 'Average prices')\n",
    "plt.plot(df_plot['p_monopoly'], color = 'red', label = 'Monopoly price')\n",
    "plt.plot(df_plot['p_nash'], color = 'green', label = 'Nash price')\n",
    "plt.xlabel('Timesteps')\n",
    "plt.ylabel('Prices')\n",
    "plt.legend()\n",
    "plt.savefig('plot.png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize = (12, 4))\n",
    "plt.plot(df_plot['avg_rewards'], label = 'Average profits')\n",
    "plt.plot(df_plot['pi_N'], label = 'Nash profits', color = 'green')\n",
    "plt.plot(df_plot['pi_M'], label = 'Monopoly profits', color = 'red')\n",
    "plt.xlabel('Timesteps')\n",
    "plt.ylabel('Profits')\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize = (12, 4))\n",
    "plt.plot(df_plot['delta'], label = 'Average profits')\n",
    "plt.axhline(1, color = 'red', label = 'Nash profits')\n",
    "plt.axhline(0, color = 'green', label = 'Monoply profits')\n",
    "plt.xlabel('Timesteps')\n",
    "plt.ylabel('Delta')\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tesis",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
